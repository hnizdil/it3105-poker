\documentclass[%
%a5paper,							% alle weiteren Papierformat einstellbar
%landscape,						% Querformat
12pt,								% Schriftgröße (12pt, 11pt (Standard))
%BCOR1cm,							% Bindekorrektur, bspw. 1 cm
%DIVcalc,							% führt die Satzspiegelberechnung neu aus
%											  s. scrguide 2.4
%twoside,							% Doppelseiten
%twocolumn,						% zweispaltiger Satz
%halfparskip*,				% Absatzformatierung s. scrguide 3.1
%headsepline,					% Trennline zum Seitenkopf	
%footsepline,					% Trennline zum Seitenfuß
titlepage						% Titelei auf eigener Seite
%normalheadings,			% Überschriften etwas kleiner (smallheadings)
%idxtotoc,						% Index im Inhaltsverzeichnis
%liststotoc,					% Abb.- und Tab.verzeichnis im Inhalt
%bibtotoc,						% Literaturverzeichnis im Inhalt
%abstracton,					% Überschrift über der Zusammenfassung an	
%leqno,   						% Nummerierung von Gleichungen links
%fleqn,								% Ausgabe von Gleichungen linksbündig
%draft								% überlangen Zeilen in Ausgabe gekennzeichnet
]
{scrartcl}

%\pagestyle{empty}	
%\pagestyle{headings}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[ansinew]{inputenc}

%\usepackage{lmodern}

\usepackage{graphicx} 
\usepackage{booktabs} 
%\usepackage{subfig} 
\usepackage{color}
\usepackage{listings}

\begin{document}
\lstset{		language=Java,
				basicstyle=\ttfamily,
				showstringspaces=false,
				commentstyle=\itshape\color[rgb]{0.2,0.6,0.2},
				%numbers=left,
				keywordstyle=\bfseries\color[rgb]{0.6,0.0,0.4},
				tabsize=4
				}
\pagestyle{empty}

%\titlehead{Titelkopf}
\subject{Report}
\title{AI Poker Players}
\subtitle{Course IT3105}
\author{Robert Braunschweig \and Jan Hn\'{i}zdil}
%\and{Der Name des Co-Autoren}
%\thanks{Fußnote}		
%\date{}						
%\publishers{Herausgeber}

\maketitle 


%\begin{abstract}


%\end{abstract}
\tableofcontents		
%\listoftables			
%\listoffigures				
\newpage

\section{Introduction}

The following text is a report for the Poker project in the IT3105 Artificial Intelligence course. The goal of the project was to write a poker simulator for Texas Hold'Em Poker which provides a simple game interface and the ability to hold human as well as artificial players. These artificial players, the bots, should be able to make reasonable decisions in the game. To show the difference of certain AI-strategies it was required to built up three different classes of bots. In the first phase the bot should make simple decisions according to the current game state, which contains values like the pot and the current bet.
The second bots should behave a little bit more sophisticated. Thus he had to  rely on values of a so called pre-flop rollout simulation, done previously to the run of the Poker game, and hand-strength calculations, done at runtime. Actually, it was ought to build a third bot with an opponent modelling feature, but this is not part of that report since we were not able to implement it in time.

The implementation of the project is done in Java, which was chosen according to it's good capabilities in object-orientation, since the content of the project was quite practical.

The report will show up the structure and implementation issues on the different parts of the project. In addition o that the results of the bot's behaviour is discussed.

\section{Basic Approach}

In the first sketch of the project we decided to model the basic elements of a poker game as classes. Thus we have the classes \texttt{Card, Game, Player}. These main classes are surrounded by several smaller classes.

The class \texttt{Card} provides obviously a model for playing cards. Thus it must have a value and a suit which are implemented with the Java construct \texttt{enum}. For public access \texttt{Card} provides methods for calculating the highest hand power (5 cards) out of 5 to 7 cards and in addition methods for generating and shuffling a card deck. For that it uses several internal (private) methods, for example, for finding a Straight. These part is quite similar to the implementation in the helper code. For details we refer to the comments in the source code.

The two other main classes are described later in the text.

\section{The Game Simulator}

The core of the game is the class \texttt{Game}. Since an execution of the Poker simulator should have only one game, the singleton pattern is realized on it. An instance of \texttt{Game} need to hold references to all participating players, a deck and the community cards. In addition has variables for the pot, the number of played games, the current bet, the highest possible bet, the blind and a \texttt{Comparator} for hands.

Within the instancing of the singleton the method \texttt{initGame} is called,
which collects all important parameters to start the game like the number of
players, which player is human or not and the number of games to play. These
information might be typed in interactively or be written from a file. After
that the game starts with the method \texttt{start} which contains a loop of
setting up a new game (\texttt{newGame}) and let it run (\texttt{runGame}).
\texttt{newGame} initializes the game state, gets a new deck and reorders the
players. The real game happens in \texttt{runGame}. In the beginning it gets the
blind from the first player in the queue. Then the first betting round starts.
Since \texttt{Game} has also a reference to all active players (players who did
not fold, yet) it can loop through all actives and asks them for their action
(fold, call, raise). This happens via the common interface of all players
\texttt{performAction}. That method returns a value of the \texttt{enum} \texttt{Action}. With that action you can switch on what should happen after performing the action. If a player folded he is removed from the active players list. If he raises the new bet is printed on the screen. A betting runs as long all active players bet the same amount (in our program a player may raise only once in a betting round). After a betting round it is checked if there is more than one active player left. In that case the community cards are dealt, else that last player gets the pot (\texttt{assignPot}). After the River a last betting round is performed, and if there are more than one players left it comes to showdown in the \texttt{getWinner} method. After the assignment of the pot to the winner/winners the game is over and a new one starts with \texttt{newGame}. For the detailed implementation of the mentioned helper methods take a look at the comments in the source code.

\section{AI Poker Player}

\subsection{Phase I Player}

\subsubsection{Structure}

We call phase I player BadBot and as every player in our game, he extends the
\texttt{Player} class. BadBot has only one method, which decides on action to take. He
differentiates between pre-flop and post-flop phases. In pre-flop phase, the bot
just calls. Unlike GoodBot (discussed later), bad one doesn't utilize any
pre-flop rollout simulation data.

After the flop cards have come out, bot can calculate its hand power, which is
done by the \texttt{calcCardsPower()} method in \texttt{Card} class. This method
finds the strongest hand in the supplied cards and returns array of integers as
proposed in the paper. The first number in the array represents the type of hand
-- one for high card, two for one pair up to nine for a straight flush. If the
bot has weaker hand than one pair (first number is less than two) he folds. On
the other hand, when having at least straight (first number bigger than four),
bot raises to the random amount which is between his own bet and maximal round
bet. He's allowed to raise only once per round, so he just calls if raised
already. In the remaining cases bot calls to stay in the game. Also when he
stays alone in the game, call occur so he can have the pot.

In conclusion our BadBot doesn't do anything fancy and only tries to obey the
basic rules of Texas Hold 'Em poker game.

\subsubsection{Testing}

We did basic testing of bots by letting them play. \texttt{Game} class takes
care of running the game by dealing the cards, running game rounds and splitting
the pot at the end. Game queries the active players about their actions during
round and decides on winner at the end of the final round.

Game can take two parameters. First set how many games should players play and
the second one is the name of the file with the initial commands. Those commands
set up the game at the beginning which involves number of players, initial
budget of players, max bet per round, etc. Game simulation is described better
in \textit{The Game Simulator} section.

As proposed in the project paper, we ran five different 1000-hand runs of game
eahc involving four BadBot players. Command used to run one 1000-hand run looks
like \texttt{java Game 1000 game-p1.txt}, where \texttt{game-p1.txt} is the file
containing game initialization commands. Results of these runs are summarized in
following five tables. Initial budget of each player was 10\,000 money with 500
being max bet per round.

\begin{center}

\vspace{5pt}
\begin{tabular}{lrr}
\toprule
\multicolumn{3}{c}{First 1000-hand run} \\
Player & Wins & Final budget \\
\midrule
BAD1 & 140 &  $-9\,922$ \\
BAD2 & 149 &  $91\,520$ \\
BAD3 & 126 & $-71\,487$ \\
BAD4 & 149 &  $29\,889$ \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrr}
\toprule
\multicolumn{3}{c}{Second 1000-hand run} \\
Player & Wins & Final budget \\
\midrule
BAD1 & 144 & $25\,910$ \\
BAD2 & 154 & $-61\,446$ \\
BAD3 & 147 & $-48\,897$ \\
BAD4 & 166 & $124\,425$ \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrr}
\toprule
\multicolumn{3}{c}{Third 1000-hand run} \\
Player & Wins & Final budget \\
\midrule
BAD1 & 147 & $143\,959$ \\
BAD2 & 155 & $182\,682$ \\
BAD3 & 142 & $-82\,924$ \\
BAD4 & 111 & $-203\,721$ \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrr}
\toprule
\multicolumn{3}{c}{Fourth 1000-hand run} \\
Player & Wins & Final budget \\
\midrule
BAD1 & 157 & $-38\,491$ \\
BAD2 & 139 & $35\,140$ \\
BAD3 & 140 & $43\,494$ \\
BAD4 & 143 & $-147$ \\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrr}
\toprule
\multicolumn{3}{c}{Fifth 1000-hand run} \\
Player & Wins & Final budget \\
\midrule
BAD1 & 141 & $30\,258$ \\
BAD2 & 132 & $36\,669$ \\
BAD3 & 155 & $28\,031$ \\
BAD4 & 134 & $-54\,965$ \\
\bottomrule
\end{tabular}

\end{center}

%TODO more tables

\subsection{Phase II Player}

\subsubsection{Pre-Flop Rollout Simulation}

This simulation estimates the chances of hole card pair winning the whole game.
It's done before the flop cards are put in. We know only two hole cards and set
other players cards and community cards randomly. Now we're able to compare our
hand with other hands and see how it performs. This is just one round and since
it's based on randomness, we should run as many rounds as we can.

We could also calculate hand winning chance by concerning all possible
combinations of final card setups, but the number of these combinations is so
large, that we can't accomplish it in reasonable computation time. That's why we
do the simulation.

It's possible to decrease number of hole pairs we need to simulate by using
equivalence classes as described in the paper. Equivalence classes are of three
types:

\begin{enumerate}
\item unpaired and unsuited -- 78 combinations;
\item unpaired and suited -- 78 combinations;
\item pairs with the same value -- 13 combinations.
\end{enumerate}

That gives us 169 possible equivalence classes together. For each class, we run
at least 1000 rollout simulations recording number of wins, ties and losses.
From those numbers it's easy to calculate winning probability of the equivalence
class (and the hole card pair that's in the class). It's also essential to
differentiate probabilities by the number of players at the table. Having data
like this, player finds his hole card pair in the table entry (with the number
of players matching) and gets the winning probability.

For calculating the rollout simulations we used the cluster machine available to
NTNU students who have parallel programming course. Using MPJ (Java wrapper to
the MPI -- Message Passing Interface) we were able to run simulations parallely
on eight processors. Each processor got even part of the 169-item array of
equivalence classes and then ran simulations for each pair and number of players
going from two to ten. We managed to run almost five million runs for each
equivalence class, which gives us a little bit more then 500\,000 rollouts for
each class and each number of players.

Results of the rollouts were stored in the MySQL database. Then simple SQL query
was run to calculate probabilities per each class and each number of players.
Results of this query were stored into textfile approximately 40\,kB in size.
This file (\texttt{PreFlopTable.dat}) is used by the \texttt{PreFlopTable}
class, which pulls the probabilities out and stores them into internal Java
data structure. Our GoodBot player can then easily use the
\texttt{getWinningProbability()} method of the \texttt{PreFlopTable} class to
get winning probability for his hole pair. The method takes two arguments --
hole cards and number of players in the game.

\subsubsection{Hand-Strength Calculation}

Since the better bot shall perform hand-strength calculations after the dealing of community cards he need to have method for this. That method is called \texttt{calcHandStrength}. It takes the number of active players together with the current community cards in a list and gives back a probability of winning for the calling player. Therefore it gets all  pair-combinations out of the remaining deck with help of the class \texttt{Combination}. Every pair is merged with the community cards to a complete hand or more cards. For every set the highest power is calculated and compared with the highest power of the calling player. The result increments the according element of the integer array \texttt{statistic} (loss, tie, win). In the end the probability is calculated by the equation from the paper.

\subsubsection{Testing}

Given above techniques GoodBot player can do better decisions on which actions
to take. In the pre-flop phase he uses the hole pair winning probabilities to
decide on folding, calling or even raising. Here comes a little tricky part
which is setting the probability thresholds. If the winning probability is less
than 0.085 our bot folds. Between 0.085 and 0.2 he calls and with higher
probability he raises random amount.

In the post-flop phases our bot has more information and hopefully can do better
decisions. He uses hand strength calculated according to the above subsection.
Threshold for those phases are 0.2 and 0.5. When raising, GoodBot takes into
account his hand strength.

As paper proposed, we again ran five 1000-hand game runs involving four BadBot
and four GoodBot players.

\begin{center}

\vspace{5pt}
\begin{tabular}{lrr}
\toprule
\multicolumn{3}{c}{First 1000-hand run} \\
Player & Wins & Final budget \\
\midrule
BAD1&133&-196300\\
BAD2&115&-598414\\
BAD3&132&-1077654\\
BAD4&122&-1569593\\
GOOD1&53&850387\\
GOOD2&55&752577\\
GOOD3&67&1155566\\
GOOD4&69&763426\\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrr}
\toprule
\multicolumn{3}{c}{Second 1000-hand run} \\
Player & Wins & Final budget \\
\midrule
BAD1&119&-496395\\
BAD2&133&-409540\\
BAD3&107&-1200254\\
BAD4&135&-1457898\\
GOOD1&61&910192\\
GOOD2&54&872461\\
GOOD3&58&716223\\
GOOD4&68&1145209\\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrr}
\toprule
\multicolumn{3}{c}{Third 1000-hand run} \\
Player & Wins & Final budget \\
\midrule
BAD1&129&-858307\\
BAD2&124&-719593\\
BAD3&94&-1343100\\
BAD4&146&-1106336\\
GOOD1&66&1354768\\
GOOD2&53&611992\\
GOOD3&59&1118677\\
GOOD4&65&1021894\\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrr}
\toprule
\multicolumn{3}{c}{Fourth 1000-hand run} \\
Player & Wins & Final budget \\
\midrule
BAD1&129&-1006496\\
BAD2&120&-899251\\
BAD3&108&-654700\\
BAD4&163&-551934\\
GOOD1&60&1031790\\
GOOD2&48&409538\\
GOOD3&63&958067\\
GOOD4&63&792984\\
\bottomrule
\end{tabular}

\vspace{5pt}
\begin{tabular}{lrr}
\toprule
\multicolumn{3}{c}{Fifth 1000-hand run} \\
Player & Wins & Final budget \\
\midrule
BAD1&130&-910942\\
BAD2&112&-663131\\
BAD3&112&-1655700\\
BAD4&147&-1299193\\
GOOD1&50&700803\\
GOOD2&75&1764724\\
GOOD3&59&1198982\\
GOOD4&61&944450\\
\bottomrule
\end{tabular}

\end{center}


\section{Conclusion}
Some of the project's main issues, the pre-flop simulation and the hand-strength calculation, work properly. Other issues like the opponent modelling do not work.
One of the biggest disadvantages of the phase II bot is that he does not use the provides information from the pre-flop simulation and the hand-strength calculation very well. So at this point the behaviour of the better bot could be highly improved.

\end{document}
